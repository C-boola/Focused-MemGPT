{
  "cells": [
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "# Simple Accuracy Checker for LongMemEval Results\n",
        "\n",
        "This notebook allows you to quickly check the accuracy of any evaluation result file.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "import json\n",
        "import os\n",
        "from pathlib import Path\n",
        "import pandas as pd\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "def calculate_accuracy(filepath):\n",
        "    \"\"\"\n",
        "    Calculate accuracy from an evaluation results file.\n",
        "    \n",
        "    Args:\n",
        "        filepath (str): Path to the .eval-results file\n",
        "    \n",
        "    Returns:\n",
        "        dict: Dictionary with accuracy metrics and details\n",
        "    \"\"\"\n",
        "    try:\n",
        "        with open(filepath, 'r') as f:\n",
        "            results = [json.loads(line.strip()) for line in f if line.strip()]\n",
        "        \n",
        "        if not results:\n",
        "            return {'error': 'No results found in file'}\n",
        "        \n",
        "        # Count correct and total\n",
        "        correct = 0\n",
        "        total = len(results)\n",
        "        \n",
        "        for result in results:\n",
        "            if result.get('autoeval_label', {}).get('label', False):\n",
        "                correct += 1\n",
        "        \n",
        "        accuracy = correct / total if total > 0 else 0.0\n",
        "        \n",
        "        return {\n",
        "            'filename': os.path.basename(filepath),\n",
        "            'total_questions': total,\n",
        "            'correct_answers': correct,\n",
        "            'incorrect_answers': total - correct,\n",
        "            'accuracy': accuracy,\n",
        "            'accuracy_percentage': accuracy * 100\n",
        "        }\n",
        "    \n",
        "    except Exception as e:\n",
        "        return {'error': f'Error processing file: {str(e)}'}\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
        "def show_sample_results(filepath, num_samples=5):\n",
        "    \"\"\"\n",
        "    Show a few sample results from the file to understand the data.\n",
        "    \n",
        "    Args:\n",
        "        filepath (str): Path to the .eval-results file\n",
        "        num_samples (int): Number of samples to show\n",
        "    \"\"\"\n",
        "    try:\n",
        "        with open(filepath, 'r') as f:\n",
        "            results = [json.loads(line.strip()) for line in f if line.strip()]\n",
        "        \n",
        "        print(f\"\\nüìã Sample Results from {os.path.basename(filepath)}:\")\n",
        "        print(\"=\" * 80)\n",
        "        \n",
        "        for i, result in enumerate(results[:num_samples]):\n",
        "            label = result.get('autoeval_label', {}).get('label', False)\n",
        "            status = \"‚úÖ CORRECT\" if label else \"‚ùå INCORRECT\"\n",
        "            \n",
        "            print(f\"\\n{i+1}. {status}\")\n",
        "            print(f\"   Question ID: {result.get('question_id', 'N/A')}\")\n",
        "            print(f\"   Ground Truth: {result.get('ground_truth', 'N/A')}\")\n",
        "            print(f\"   Hypothesis: {result.get('hypothesis', 'N/A')[:100]}...\")\n",
        "        \n",
        "        if len(results) > num_samples:\n",
        "            print(f\"\\n... and {len(results) - num_samples} more results\")\n",
        "    \n",
        "    except Exception as e:\n",
        "        print(f\"Error showing sample results: {e}\")\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## How to Use This Notebook\n",
        "\n",
        "**Option 1**: Change the `filepath` variable below to point to any specific evaluation file\n",
        "**Option 2**: Use the file finder to see all available files and pick one\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üìä Accuracy Analysis for: memgpt_hypotheses_xml_hybrid_beta0.0_cluster_20250716_223138.jsonl.eval-results-gpt-4o-mini\n",
            "============================================================\n",
            "Total Questions: 289\n",
            "Correct Answers: 140\n",
            "Incorrect Answers: 149\n",
            "Accuracy: 0.4844 (48.44%)\n"
          ]
        }
      ],
      "source": [
        "# OPTION 1: Specify a file directly\n",
        "# Change this path to any evaluation result file you want to analyze\n",
        "filepath = \"/home/samer/Documents/LAU/Research/focus_memgpt/Focused-MemGPT/paper_experiments/longmemeval/evaluation_scripts/full runs with clustering/memgpt_hypotheses_xml_hybrid_beta0.0_cluster_20250716_223138.jsonl.eval-results-gpt-4o-mini\"\n",
        "\n",
        "# Calculate accuracy\n",
        "result = calculate_accuracy(filepath)\n",
        "\n",
        "if 'error' in result:\n",
        "    print(f\"‚ùå Error: {result['error']}\")\n",
        "else:\n",
        "    print(f\"üìä Accuracy Analysis for: {result['filename']}\")\n",
        "    print(\"=\" * 60)\n",
        "    print(f\"Total Questions: {result['total_questions']}\")\n",
        "    print(f\"Correct Answers: {result['correct_answers']}\")\n",
        "    print(f\"Incorrect Answers: {result['incorrect_answers']}\")\n",
        "    print(f\"Accuracy: {result['accuracy']:.4f} ({result['accuracy_percentage']:.2f}%)\")\n",
        "    \n",
        "    # # Show some sample results\n",
        "    # show_sample_results(filepath, 3)\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## Option 2: Find All Available Files\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Found 39 evaluation result files:\n",
            "============================================================\n",
            " 1. ./archived/memgpt_hypotheses.jsonl.eval-results-gpt-4o-mini\n",
            " 2. ./archived/memgpt_hypotheses_fifo.jsonl.eval-results-gpt-4o-mini\n",
            " 3. ./archived/memgpt_hypotheses_hybrid.jsonl.eval-results-gpt-4o-mini\n",
            " 4. ./archived/memgpt_hypotheses_hybrid_beta1.0.jsonl.eval-results-gpt-4o-mini\n",
            " 5. ./archived/memgpt_hypotheses_prompted_hybrid_beta1.0_20250715_122759.jsonl.eval-results-gpt-4o-mini\n",
            " 6. ./archived/memgpt_hypotheses_prompted_hybrid_beta1.0_test.jsonl.eval-results-gpt-4o-mini\n",
            " 7. ./archived/memgpt_hypotheses_prompted_hybrid_beta1.0_test_20250714_222150.jsonl.eval-results-gpt-4o-mini\n",
            " 8. ./archived/memgpt_hypotheses_prompted_hybrid_beta1.0_test_20250714_223803.jsonl.eval-results-gpt-4o-mini\n",
            " 9. ./archived/old_beta/memgpt_hypotheses_hybrid_beta0.0.jsonl.eval-results-gpt-4o-mini\n",
            "10. ./archived/old_beta/memgpt_hypotheses_hybrid_beta0.1.jsonl.eval-results-gpt-4o-mini\n",
            "11. ./archived/old_beta/memgpt_hypotheses_hybrid_beta0.2.jsonl.eval-results-gpt-4o-mini\n",
            "12. ./archived/old_beta/memgpt_hypotheses_hybrid_beta0.3.jsonl.eval-results-gpt-4o-mini\n",
            "13. ./archived/old_beta/memgpt_hypotheses_hybrid_beta0.4.jsonl.eval-results-gpt-4o-mini\n",
            "14. ./archived/old_beta/memgpt_hypotheses_hybrid_beta0.6.jsonl.eval-results-gpt-4o-mini\n",
            "15. ./archived/old_beta/memgpt_hypotheses_hybrid_beta0.7.jsonl.eval-results-gpt-4o-mini\n",
            "16. ./archived/old_beta/memgpt_hypotheses_hybrid_beta0.8.jsonl.eval-results-gpt-4o-mini\n",
            "17. ./archived/old_beta/memgpt_hypotheses_hybrid_beta0.9.jsonl.eval-results-gpt-4o-mini\n",
            "18. ./archived/old_beta/memgpt_hypotheses_hybrid_beta1.0.jsonl.eval-results-gpt-4o-mini\n",
            "19. ./full runs with clustering/memgpt_hypotheses_xml_hybrid_beta0.0_cluster_20250716_223138.jsonl.eval-results-gpt-4o-mini\n",
            "20. ./full runs with clustering/memgpt_hypotheses_xml_hybrid_beta0.3_cluster_20250717_141225.jsonl.eval-results-gpt-4o-mini\n",
            "21. ./full runs with clustering/memgpt_hypotheses_xml_hybrid_beta0.5_cluster_20250716_223146.jsonl.eval-results-gpt-4o-mini\n",
            "22. ./full runs with clustering/memgpt_hypotheses_xml_hybrid_beta0.7_cluster_20250716_223825.jsonl.eval-results-gpt-4o-mini\n",
            "23. ./full runs with clustering/memgpt_hypotheses_xml_hybrid_beta1.0_cluster_20250716_185159.jsonl.eval-results-gpt-4o\n",
            "24. ./full runs with clustering/memgpt_hypotheses_xml_hybrid_beta1.0_cluster_20250716_185159.jsonl.eval-results-gpt-4o-mini\n",
            "25. ./full runs without clustering/memgpt_hypotheses_xml_hybrid_beta0.0_20250717_143327.jsonl.eval-results-gpt-4o-mini\n",
            "26. ./full runs without clustering/memgpt_hypotheses_xml_hybrid_beta1.0_20250717_143242.jsonl.eval-results-gpt-4o-mini\n",
            "27. ./inverted focus/memgpt_hypotheses_xml_hybrid_beta1.0_cluster_centroid_inverted_focus_test_20250718_201614.jsonl.eval-results-gpt-4o-mini\n",
            "28. ./memgpt_hypotheses_xml_hybrid_beta0.5_cluster_centroid_inverted_focus_20250718_221542.jsonl.eval-results-gpt-4o-mini\n",
            "29. ./memgpt_hypotheses_xml_hybrid_beta0.5_cluster_medoid_test_20250718_191005.jsonl.eval-results-gpt-4o-mini\n",
            "30. ./memgpt_hypotheses_xml_hybrid_beta1.0_cluster_centroid_inverted_focus_20250718_221518.jsonl.eval-results-gpt-4o-mini\n",
            "31. ./tests with clustering/memgpt_hypotheses_xml_hybrid_beta0.0_cluster_medoid_test_20250718_135006.jsonl.eval-results-gpt-4o-mini\n",
            "32. ./tests with clustering/memgpt_hypotheses_xml_hybrid_beta0.0_cluster_test_20250716_190251.jsonl.eval-results-gpt-4o-mini\n",
            "33. ./tests with clustering/memgpt_hypotheses_xml_hybrid_beta0.0_cluster_test_20250718_124754.jsonl.eval-results-gpt-4o-mini\n",
            "34. ./tests with clustering/memgpt_hypotheses_xml_hybrid_beta1.0_cluster_medoid_test_20250718_134850.jsonl.eval-results-gpt-4o-mini\n",
            "35. ./tests with clustering/memgpt_hypotheses_xml_hybrid_beta1.0_cluster_test_20250716_133049.jsonl.eval-results-gpt-4o-mini\n",
            "36. ./tests with clustering/memgpt_hypotheses_xml_hybrid_beta1.0_cluster_test_20250718_124734.jsonl.eval-results-gpt-4o-mini\n",
            "37. ./tests with clustering/memgpt_hypotheses_xml_hybrid_cluster_test_20250716_190313.jsonl.eval-results-gpt-4o-mini\n",
            "38. ./tests without clustering/memgpt_hypotheses_memgpt_default_hybrid_beta1.0_test_20250716_190051.jsonl.eval-results-gpt-4o-mini\n",
            "39. ./tests without clustering/memgpt_hypotheses_xml_focus_test_20250716_132843.jsonl.eval-results-gpt-4o-mini\n",
            "\n",
            "üìà Quick Overview of All Files:\n",
            "============================================================\n",
            "                                                                                                                    File  Questions  Correct Accuracy\n",
            "                                                                        memgpt_hypotheses.jsonl.eval-results-gpt-4o-mini        289      141    48.8%\n",
            "                                                                   memgpt_hypotheses_fifo.jsonl.eval-results-gpt-4o-mini        289       62    21.5%\n",
            "                                                                 memgpt_hypotheses_hybrid.jsonl.eval-results-gpt-4o-mini        289       61    21.1%\n",
            "                                                         memgpt_hypotheses_hybrid_beta1.0.jsonl.eval-results-gpt-4o-mini        289      113    39.1%\n",
            "                                memgpt_hypotheses_prompted_hybrid_beta1.0_20250715_122759.jsonl.eval-results-gpt-4o-mini        289      141    48.8%\n",
            "                                           memgpt_hypotheses_prompted_hybrid_beta1.0_test.jsonl.eval-results-gpt-4o-mini         10        6    60.0%\n",
            "                           memgpt_hypotheses_prompted_hybrid_beta1.0_test_20250714_222150.jsonl.eval-results-gpt-4o-mini         10        7    70.0%\n",
            "                           memgpt_hypotheses_prompted_hybrid_beta1.0_test_20250714_223803.jsonl.eval-results-gpt-4o-mini         10        6    60.0%\n",
            "                                                         memgpt_hypotheses_hybrid_beta0.0.jsonl.eval-results-gpt-4o-mini        289       62    21.5%\n",
            "                                                         memgpt_hypotheses_hybrid_beta0.1.jsonl.eval-results-gpt-4o-mini        289       68    23.5%\n",
            "                                                         memgpt_hypotheses_hybrid_beta0.2.jsonl.eval-results-gpt-4o-mini        289       66    22.8%\n",
            "                                                         memgpt_hypotheses_hybrid_beta0.3.jsonl.eval-results-gpt-4o-mini        289       66    22.8%\n",
            "                                                         memgpt_hypotheses_hybrid_beta0.4.jsonl.eval-results-gpt-4o-mini        289       60    20.8%\n",
            "                                                         memgpt_hypotheses_hybrid_beta0.6.jsonl.eval-results-gpt-4o-mini        289       61    21.1%\n",
            "                                                         memgpt_hypotheses_hybrid_beta0.7.jsonl.eval-results-gpt-4o-mini        289       61    21.1%\n",
            "                                                         memgpt_hypotheses_hybrid_beta0.8.jsonl.eval-results-gpt-4o-mini        289       62    21.5%\n",
            "                                                         memgpt_hypotheses_hybrid_beta0.9.jsonl.eval-results-gpt-4o-mini        289       69    23.9%\n",
            "                                                         memgpt_hypotheses_hybrid_beta1.0.jsonl.eval-results-gpt-4o-mini        289       61    21.1%\n",
            "                             memgpt_hypotheses_xml_hybrid_beta0.0_cluster_20250716_223138.jsonl.eval-results-gpt-4o-mini        289      140    48.4%\n",
            "                             memgpt_hypotheses_xml_hybrid_beta0.3_cluster_20250717_141225.jsonl.eval-results-gpt-4o-mini        289      137    47.4%\n",
            "                             memgpt_hypotheses_xml_hybrid_beta0.5_cluster_20250716_223146.jsonl.eval-results-gpt-4o-mini        289      142    49.1%\n",
            "                             memgpt_hypotheses_xml_hybrid_beta0.7_cluster_20250716_223825.jsonl.eval-results-gpt-4o-mini        289      145    50.2%\n",
            "                                  memgpt_hypotheses_xml_hybrid_beta1.0_cluster_20250716_185159.jsonl.eval-results-gpt-4o        289      142    49.1%\n",
            "                             memgpt_hypotheses_xml_hybrid_beta1.0_cluster_20250716_185159.jsonl.eval-results-gpt-4o-mini        289      142    49.1%\n",
            "                                     memgpt_hypotheses_xml_hybrid_beta0.0_20250717_143327.jsonl.eval-results-gpt-4o-mini        289      134    46.4%\n",
            "                                     memgpt_hypotheses_xml_hybrid_beta1.0_20250717_143242.jsonl.eval-results-gpt-4o-mini        289      136    47.1%\n",
            "memgpt_hypotheses_xml_hybrid_beta1.0_cluster_centroid_inverted_focus_test_20250718_201614.jsonl.eval-results-gpt-4o-mini         25       18    72.0%\n",
            "     memgpt_hypotheses_xml_hybrid_beta0.5_cluster_centroid_inverted_focus_20250718_221542.jsonl.eval-results-gpt-4o-mini        289      141    48.8%\n",
            "                 memgpt_hypotheses_xml_hybrid_beta0.5_cluster_medoid_test_20250718_191005.jsonl.eval-results-gpt-4o-mini         25       17    68.0%\n",
            "     memgpt_hypotheses_xml_hybrid_beta1.0_cluster_centroid_inverted_focus_20250718_221518.jsonl.eval-results-gpt-4o-mini        289      143    49.5%\n",
            "                 memgpt_hypotheses_xml_hybrid_beta0.0_cluster_medoid_test_20250718_135006.jsonl.eval-results-gpt-4o-mini         25       20    80.0%\n",
            "                        memgpt_hypotheses_xml_hybrid_beta0.0_cluster_test_20250716_190251.jsonl.eval-results-gpt-4o-mini         25       20    80.0%\n",
            "                        memgpt_hypotheses_xml_hybrid_beta0.0_cluster_test_20250718_124754.jsonl.eval-results-gpt-4o-mini         25       19    76.0%\n",
            "                 memgpt_hypotheses_xml_hybrid_beta1.0_cluster_medoid_test_20250718_134850.jsonl.eval-results-gpt-4o-mini         25       18    72.0%\n",
            "                        memgpt_hypotheses_xml_hybrid_beta1.0_cluster_test_20250716_133049.jsonl.eval-results-gpt-4o-mini         25       19    76.0%\n",
            "                        memgpt_hypotheses_xml_hybrid_beta1.0_cluster_test_20250718_124734.jsonl.eval-results-gpt-4o-mini         25       19    76.0%\n",
            "                                memgpt_hypotheses_xml_hybrid_cluster_test_20250716_190313.jsonl.eval-results-gpt-4o-mini         25       19    76.0%\n",
            "                     memgpt_hypotheses_memgpt_default_hybrid_beta1.0_test_20250716_190051.jsonl.eval-results-gpt-4o-mini         25       19    76.0%\n",
            "                                         memgpt_hypotheses_xml_focus_test_20250716_132843.jsonl.eval-results-gpt-4o-mini         25       17    68.0%\n"
          ]
        }
      ],
      "source": [
        "# Find all evaluation result files\n",
        "eval_files = []\n",
        "\n",
        "for root, dirs, files in os.walk('.'):\n",
        "    for file in files:\n",
        "        if file.endswith('.eval-results-gpt-4o-mini') or file.endswith('.eval-results-gpt-4o'):\n",
        "            eval_files.append(os.path.join(root, file))\n",
        "\n",
        "eval_files = sorted(eval_files)\n",
        "\n",
        "print(f\"Found {len(eval_files)} evaluation result files:\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "for i, filepath in enumerate(eval_files):\n",
        "    print(f\"{i+1:2d}. {filepath}\")\n",
        "\n",
        "# Quick analysis of all files\n",
        "if eval_files:\n",
        "    print(f\"\\nüìà Quick Overview of All Files:\")\n",
        "    print(\"=\" * 60)\n",
        "    \n",
        "    all_results = []\n",
        "    for filepath in eval_files:\n",
        "        result = calculate_accuracy(filepath)\n",
        "        if 'error' not in result:\n",
        "            all_results.append({\n",
        "                'File': os.path.basename(filepath),\n",
        "                'Questions': result['total_questions'],\n",
        "                'Correct': result['correct_answers'],\n",
        "                'Accuracy': f\"{result['accuracy_percentage']:.1f}%\"\n",
        "            })\n",
        "    \n",
        "    if all_results:\n",
        "        df = pd.DataFrame(all_results)\n",
        "        print(df.to_string(index=False))\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## Option 3: Analyze Specific File by Number\n",
        "\n",
        "Change the `file_number` below to select a file from the list above:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Change this number to select different files (1-based indexing)\n",
        "file_number = 1\n",
        "\n",
        "if eval_files and 1 <= file_number <= len(eval_files):\n",
        "    filepath = eval_files[file_number - 1]\n",
        "    \n",
        "    # Calculate accuracy\n",
        "    result = calculate_accuracy(filepath)\n",
        "    \n",
        "    if 'error' in result:\n",
        "        print(f\"‚ùå Error: {result['error']}\")\n",
        "    else:\n",
        "        print(f\"üìä Detailed Analysis for File #{file_number}:\")\n",
        "        print(f\"üìÅ {result['filename']}\")\n",
        "        print(\"=\" * 60)\n",
        "        print(f\"Total Questions: {result['total_questions']}\")\n",
        "        print(f\"Correct Answers: {result['correct_answers']}\")\n",
        "        print(f\"Incorrect Answers: {result['incorrect_answers']}\")\n",
        "        print(f\"Accuracy: {result['accuracy']:.4f} ({result['accuracy_percentage']:.2f}%)\")\n",
        "        \n",
        "        # Show sample results\n",
        "        show_sample_results(filepath, 5)\n",
        "else:\n",
        "    print(f\"‚ùå Invalid file number. Please choose between 1 and {len(eval_files) if eval_files else 0}\")\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "focus_memgpt",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
